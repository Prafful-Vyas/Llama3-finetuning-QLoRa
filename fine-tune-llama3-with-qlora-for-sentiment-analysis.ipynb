{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fine-tune Llama 3 with QLoRA for Sentiment Analysis</h1>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this notebook, we will experiment with fine-tuning a Llama 3 model, using quanitzation and low-rank-adaptation combined, i.e. QLoRA.\n",
    "Here are the steps:\n",
    "\n",
    "* Load a dataset with sentiments  \n",
    "* Process it to create balanced train, evaluation and test sets and prepare prompts for LLM\n",
    "* Prepare quantization config and load tokenizer and model using Transformers\n",
    "* Define evaluation function\n",
    "* Test Llama3 without fine-tuning with prediction of sentiments\n",
    "* Define fine-tune configuration, parameters and trainer\n",
    "* Start fine-tuning using SFTTrainer\n",
    "* Evaluate (using the test set) the fine-tuned model\n",
    "\n",
    "## Data used\n",
    "\n",
    "The dataset used is [Financial Sentiment Analysis](https://www.kaggle.com/datasets/sbhatti/financial-sentiment-analysis). It combines two sources of data (FiQA, Financial PhraseBank), providing financial sentences, with sentiments added. It can be used to adapt models for sentiment analysis with specialization in financial domain. \n",
    "\n",
    "\n",
    "## What is Llama3?\n",
    "Llama3 is the latest release of open-source LLMs from Meta, with features pretrained and instruction-fine-tuned language models with 8B and 70B parameters.\n",
    "\n",
    "## What is LoRA?\n",
    "LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times.\n",
    "\n",
    "## What is QLoRA?\n",
    "QLoRA builds on LoRA by incorporating quantization techniques to further reduce memory usage while maintaining, or even enhancing, model performance. With QLoRA it is possible to finetune a 70B parameter model that requires 36 GPUs with only 2!\n",
    "\n",
    "## What is PEFT?\n",
    "Parameter-efficient Fine-tuning (PEFT) is a technique used in Natural Language Processing (NLP) to improve the performance of pre-trained language models on specific downstream tasks. It involves reusing the pre-trained modelâ€™s parameters and fine-tuning them on a smaller dataset, which saves computational resources and time compared to training the entire model from scratch. PEFT achieves this efficiency by freezing some of the layers of the pre-trained model and only fine-tuning the last few layers that are specific to the downstream task.\n",
    "\n",
    "## What is SFTTrainer?\n",
    "SFT in SFTTrainer stands for supervised fine-tuning. The trl (Transformer Reinforcement Learning) library from HuggingFace provides a simple API to fine-tune models using SFTTrainer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will install the resources needed for importing the libraries (with the correct versions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:48:44.899201Z",
     "iopub.status.busy": "2024-11-16T18:48:44.898382Z",
     "iopub.status.idle": "2024-11-16T18:50:48.036066Z",
     "shell.execute_reply": "2024-11-16T18:50:48.034998Z",
     "shell.execute_reply.started": "2024-11-16T18:48:44.899153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install -q -U transformers==\"4.40.0\"\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:50:48.038918Z",
     "iopub.status.busy": "2024-11-16T18:50:48.038412Z",
     "iopub.status.idle": "2024-11-16T18:50:48.044075Z",
     "shell.execute_reply": "2024-11-16T18:50:48.043188Z",
     "shell.execute_reply.started": "2024-11-16T18:50:48.038867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:50:48.045642Z",
     "iopub.status.busy": "2024-11-16T18:50:48.045246Z",
     "iopub.status.idle": "2024-11-16T18:50:48.246184Z",
     "shell.execute_reply": "2024-11-16T18:50:48.245352Z",
     "shell.execute_reply.started": "2024-11-16T18:50:48.045581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:50:48.249165Z",
     "iopub.status.busy": "2024-11-16T18:50:48.248841Z",
     "iopub.status.idle": "2024-11-16T18:51:08.798381Z",
     "shell.execute_reply": "2024-11-16T18:51:08.797416Z",
     "shell.execute_reply.started": "2024-11-16T18:50:48.249133Z"
    },
    "papermill": {
     "duration": 14.485002,
     "end_time": "2023-10-16T11:00:18.917449",
     "exception": false,
     "start_time": "2023-10-16T11:00:04.432447",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:51:08.800392Z",
     "iopub.status.busy": "2024-11-16T18:51:08.800013Z",
     "iopub.status.idle": "2024-11-16T18:51:08.805672Z",
     "shell.execute_reply": "2024-11-16T18:51:08.80468Z",
     "shell.execute_reply.started": "2024-11-16T18:51:08.80035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"pytorch version {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:51:08.807289Z",
     "iopub.status.busy": "2024-11-16T18:51:08.806933Z",
     "iopub.status.idle": "2024-11-16T18:51:08.81649Z",
     "shell.execute_reply": "2024-11-16T18:51:08.815455Z",
     "shell.execute_reply.started": "2024-11-16T18:51:08.807248Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"working on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disabling two features in PyTorch related to memory efficiency and speed during operations on the Graphics Processing Unit (GPU) specifically for the scaled dot product attention (SDPA) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:51:08.818237Z",
     "iopub.status.busy": "2024-11-16T18:51:08.817731Z",
     "iopub.status.idle": "2024-11-16T18:51:08.827579Z",
     "shell.execute_reply": "2024-11-16T18:51:08.826755Z",
     "shell.execute_reply.started": "2024-11-16T18:51:08.818195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:52:15.855149Z",
     "iopub.status.busy": "2024-11-16T18:52:15.854795Z",
     "iopub.status.idle": "2024-11-16T18:52:15.871072Z",
     "shell.execute_reply": "2024-11-16T18:52:15.87016Z",
     "shell.execute_reply.started": "2024-11-16T18:52:15.855117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "filename = \"/kaggle/input/financial-sentiment-analysis/data.csv\"\n",
    "\n",
    "df = pd.read_csv(filename, \n",
    "                 encoding=\"utf-8\", encoding_errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:52:18.534596Z",
     "iopub.status.busy": "2024-11-16T18:52:18.534193Z",
     "iopub.status.idle": "2024-11-16T18:52:18.539257Z",
     "shell.execute_reply": "2024-11-16T18:52:18.538339Z",
     "shell.execute_reply.started": "2024-11-16T18:52:18.534558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.columns = [\"text\", \"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:52:20.131167Z",
     "iopub.status.busy": "2024-11-16T18:52:20.130412Z",
     "iopub.status.idle": "2024-11-16T18:52:20.140147Z",
     "shell.execute_reply": "2024-11-16T18:52:20.139284Z",
     "shell.execute_reply.started": "2024-11-16T18:52:20.131131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from each type of sentiment a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:53:32.004102Z",
     "iopub.status.busy": "2024-11-16T18:53:32.003694Z",
     "iopub.status.idle": "2024-11-16T18:53:33.541451Z",
     "shell.execute_reply": "2024-11-16T18:53:33.540667Z",
     "shell.execute_reply.started": "2024-11-16T18:53:32.004067Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = list()\n",
    "X_test = list()\n",
    "for sentiment in [\"positive\", \"neutral\", \"negative\"]:\n",
    "    train, test  = train_test_split(df[df.sentiment==sentiment], \n",
    "                                    train_size=300,\n",
    "                                    test_size=300, \n",
    "                                    random_state=42)\n",
    "    X_train.append(train)\n",
    "    X_test.append(test)\n",
    "\n",
    "X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
    "X_test = pd.concat(X_test)\n",
    "\n",
    "eval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\n",
    "X_eval = df[df.index.isin(eval_idx)]\n",
    "X_eval = (X_eval\n",
    "          .groupby('sentiment', group_keys=False)\n",
    "          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n",
    "X_train = X_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we got this right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:54:57.121583Z",
     "iopub.status.busy": "2024-11-16T18:54:57.120609Z",
     "iopub.status.idle": "2024-11-16T18:54:57.128647Z",
     "shell.execute_reply": "2024-11-16T18:54:57.127535Z",
     "shell.execute_reply.started": "2024-11-16T18:54:57.121544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_eval.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:55:05.810727Z",
     "iopub.status.busy": "2024-11-16T18:55:05.810327Z",
     "iopub.status.idle": "2024-11-16T18:55:05.816194Z",
     "shell.execute_reply": "2024-11-16T18:55:05.815272Z",
     "shell.execute_reply.started": "2024-11-16T18:55:05.810691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n",
    "\n",
    "            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n",
    "            \"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            Analyze the sentiment of the news headline enclosed in square brackets, \n",
    "            determine if it is positive, neutral, or negative, and return the answer as \n",
    "            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n",
    "\n",
    "            [{data_point[\"text\"]}] = \"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:55:10.233943Z",
     "iopub.status.busy": "2024-11-16T18:55:10.233051Z",
     "iopub.status.idle": "2024-11-16T18:55:10.294923Z",
     "shell.execute_reply": "2024-11-16T18:55:10.29403Z",
     "shell.execute_reply.started": "2024-11-16T18:55:10.233901Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n",
    "                       columns=[\"text\"])\n",
    "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n",
    "                      columns=[\"text\"])\n",
    "\n",
    "y_true = X_test.sentiment\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
    "\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "eval_data = Dataset.from_pandas(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a function to evaluate the results from our fine-tuned sentiment model. The function performs the following steps:\n",
    "\n",
    "1. Maps the sentiment labels to a numerical representation, where 2 represents positive, 1 represents neutral, and 0 represents negative.\n",
    "2. Calculates the accuracy of the model on the test data.\n",
    "3. Generates an accuracy report for each sentiment label.\n",
    "4. Generates a classification report for the model.\n",
    "5. Generates a confusion matrix for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T20:56:15.397593Z",
     "iopub.status.busy": "2024-11-16T20:56:15.396724Z",
     "iopub.status.idle": "2024-11-16T20:56:15.408535Z",
     "shell.execute_reply": "2024-11-16T20:56:15.407327Z",
     "shell.execute_reply.started": "2024-11-16T20:56:15.397553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    labels = ['positive', 'neutral', 'negative']\n",
    "    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    y_true = np.vectorize(map_func)(y_true)\n",
    "    y_pred = np.vectorize(map_func)(y_pred)\n",
    "\n",
    "    # Class labels\n",
    "    class_labels = ['negative', 'neutral', 'positive']\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred, target_names=class_labels)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(6, 5))\n",
    "   \n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Oranges\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Llama3 model without fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to take care of the model, which is a 8b-chat-hf (8 billion parameters, no RLHF, in the HuggingFace compatible format), loading from Kaggle models and quantization.\n",
    "\n",
    "Model loading and quantization:\n",
    "\n",
    "* First the code loads the Llama-3 language model from the Kaggle Models.\n",
    "* Then the code gets the float16 data type from the torch library. This is the data type that will be used for the computations.\n",
    "* Next, it creates a BitsAndBytesConfig object with the following settings:\n",
    "    1. load_in_4bit: Load the model weights in 4-bit format.\n",
    "    2. bnb_4bit_quant_type: Use the \"nf4\" quantization type. 4-bit NormalFloat (NF4), is a new data type that is information theoretically optimal for normally distributed weights.\n",
    "    3. bnb_4bit_compute_dtype: Use the float16 data type for computations.\n",
    "    4. bnb_4bit_use_double_quant: Do not use double quantization (reduces the average memory footprint by quantizing also the quantization constants and saves an additional 0.4 bits per parameter.).\n",
    "* Then the code creates a AutoModelForCausalLM object from the pre-trained Llama-2 language model, using the BitsAndBytesConfig object for quantization.\n",
    "* After that, the code disables caching for the model.\n",
    "* Finally the code sets the pre-training token probability to 1.\n",
    "\n",
    "Tokenizer loading:\n",
    "\n",
    "* First, the code loads the tokenizer for the Llama-3 language model.\n",
    "* Then it sets the padding token to be the end-of-sequence (EOS) token.\n",
    "* Finally, the code sets the padding side to be \"right\", which means that the input sequences will be padded on the right side. This is crucial for correct padding direction (this is the way with Llama 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T18:55:39.969799Z",
     "iopub.status.busy": "2024-11-16T18:55:39.969049Z",
     "iopub.status.idle": "2024-11-16T18:56:55.832701Z",
     "shell.execute_reply": "2024-11-16T18:56:55.831742Z",
     "shell.execute_reply.started": "2024-11-16T18:55:39.969761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"../input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "max_seq_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_seq_length=max_seq_length)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we set a function for predicting the sentiment of a news headline using the Llama-3 language model. The function takes three arguments:\n",
    "\n",
    "test: A Pandas DataFrame containing the news headlines to be predicted.\n",
    "model: The pre-trained Llama-3 language model.\n",
    "tokenizer: The tokenizer for the Llama-3 language model.\n",
    "\n",
    "The function works as follows:\n",
    "\n",
    "1. For each news headline in the test DataFrame:\n",
    "    * Create a prompt for the language model, which asks it to analyze the sentiment of the news headline and return the corresponding sentiment label.\n",
    "    * Use the pipeline() function from the Hugging Face Transformers library to generate text from the language model, using the prompt.\n",
    "    * Extract the predicted sentiment label from the generated text.\n",
    "    * Append the predicted sentiment label to the y_pred list.\n",
    "2. Return the y_pred list.\n",
    "\n",
    "The pipeline() function from the Hugging Face Transformers library is used to generate text from the language model. The task argument specifies that the task is text generation. The model and tokenizer arguments specify the pre-trained Llama-2 language model and the tokenizer for the language model. The max_new_tokens argument specifies the maximum number of new tokens to generate. The temperature argument controls the randomness of the generated text. A lower temperature will produce more predictable text, while a higher temperature will produce more creative and unexpected text.\n",
    "\n",
    "The if statement checks if the generated text contains the word \"positive\". If it does, then the predicted sentiment label is \"positive\". Otherwise, the if statement checks if the generated text contains the word \"negative\". If it does, then the predicted sentiment label is \"negative\". Otherwise, the if statement checks if the generated text contains the word \"neutral\". If it does, then the predicted sentiment label is \"neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T19:02:19.611125Z",
     "iopub.status.busy": "2024-11-16T19:02:19.610049Z",
     "iopub.status.idle": "2024-11-16T19:02:19.618177Z",
     "shell.execute_reply": "2024-11-16T19:02:19.617206Z",
     "shell.execute_reply.started": "2024-11-16T19:02:19.611082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i][\"text\"]\n",
    "        pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens = 1, \n",
    "                        temperature = 0.0,\n",
    "                       )\n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\"=\")[-1]\n",
    "        if \"positive\" in answer:\n",
    "            y_pred.append(\"positive\")\n",
    "        elif \"negative\" in answer:\n",
    "            y_pred.append(\"negative\")\n",
    "        elif \"neutral\" in answer:\n",
    "            y_pred.append(\"neutral\")\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are ready to test the Llama 3 8b-chat-hf model and see how it performs on our problem without any fine-tuning. This allows us to get insights on the model itself and establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T19:02:22.401739Z",
     "iopub.status.busy": "2024-11-16T19:02:22.400943Z",
     "iopub.status.idle": "2024-11-16T19:07:37.752007Z",
     "shell.execute_reply": "2024-11-16T19:07:37.751226Z",
     "shell.execute_reply.started": "2024-11-16T19:02:22.401701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we evaluate the results. There is little to be said, it is performing really terribly because the 8b-chat-hf model tends to just predict a neutral sentiment and seldom it detects positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T19:07:37.75413Z",
     "iopub.status.busy": "2024-11-16T19:07:37.753824Z",
     "iopub.status.idle": "2024-11-16T19:07:37.776437Z",
     "shell.execute_reply": "2024-11-16T19:07:37.774896Z",
     "shell.execute_reply.started": "2024-11-16T19:07:37.754099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Llam3 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PEFTConfig:\n",
    "\n",
    "The peft_config object specifies the parameters for PEFT. The following are some of the most important parameters:\n",
    "\n",
    "* lora_alpha: The learning rate for the LoRA update matrices.\n",
    "* lora_dropout: The dropout probability for the LoRA update matrices.\n",
    "* r: The rank of the LoRA update matrices.\n",
    "* bias: The type of bias to use. The possible values are none, additive, and learned.\n",
    "* task_type: The type of task that the model is being trained for. The possible values are CAUSAL_LM and MASKED_LM.\n",
    "\n",
    "TrainingArguments:\n",
    "\n",
    "The training_arguments object specifies the parameters for training the model. The following are some of the most important parameters:\n",
    "\n",
    "* output_dir: The directory where the training logs and checkpoints will be saved.\n",
    "* num_train_epochs: The number of epochs to train the model for.\n",
    "* per_device_train_batch_size: The number of samples in each batch on each device.\n",
    "* gradient_accumulation_steps: The number of batches to accumulate gradients before updating the model parameters.\n",
    "* optim: The optimizer to use for training the model.\n",
    "* save_steps: The number of steps after which to save a checkpoint.\n",
    "* logging_steps: The number of steps after which to log the training metrics.\n",
    "* learning_rate: The learning rate for the optimizer.\n",
    "* weight_decay: The weight decay parameter for the optimizer.\n",
    "* fp16: Whether to use 16-bit floating-point precision.\n",
    "* bf16: Whether to use BFloat16 precision.\n",
    "* max_grad_norm: The maximum gradient norm.\n",
    "* max_steps: The maximum number of steps to train the model for.\n",
    "* warmup_ratio: The proportion of the training steps to use for warming up the learning rate.\n",
    "* group_by_length: Whether to group the training samples by length.\n",
    "* lr_scheduler_type: The type of learning rate scheduler to use.\n",
    "* report_to: The tools to report the training metrics to.\n",
    "* evaluation_strategy: The strategy for evaluating the model during training.\n",
    "\n",
    "SFTTrainer:\n",
    "\n",
    "The SFTTrainer is a custom trainer class from the TRL library. It is used to train large language models (also using the PEFT method).\n",
    "\n",
    "The SFTTrainer object is initialized with the following arguments:\n",
    "\n",
    "* model: The model to be trained.\n",
    "* train_dataset: The training dataset.\n",
    "* eval_dataset: The evaluation dataset.\n",
    "* peft_config: The PEFT configuration.\n",
    "* dataset_text_field: The name of the text field in the dataset.\n",
    "* tokenizer: The tokenizer to use.\n",
    "* args: The training arguments.\n",
    "* packing: Whether to pack the training samples.\n",
    "* max_seq_length: The maximum sequence length.\n",
    "\n",
    "Once the SFTTrainer object is initialized, it can be used to train the model by calling the train() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T19:08:07.702521Z",
     "iopub.status.busy": "2024-11-16T19:08:07.702074Z",
     "iopub.status.idle": "2024-11-16T19:08:07.710186Z",
     "shell.execute_reply": "2024-11-16T19:08:07.709183Z",
     "shell.execute_reply.started": "2024-11-16T19:08:07.702481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, \n",
    "                             recall_score, \n",
    "                             precision_score, \n",
    "                             f1_score)\n",
    "\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T19:08:14.087689Z",
     "iopub.status.busy": "2024-11-16T19:08:14.086932Z",
     "iopub.status.idle": "2024-11-16T19:08:16.735187Z",
     "shell.execute_reply": "2024-11-16T19:08:16.73442Z",
     "shell.execute_reply.started": "2024-11-16T19:08:14.087648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_dir=\"trained_weigths\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                   \n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25, \n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    #eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will train the model using the trainer.train() method and then save the trained model to the trained-model directory. Using The standard GPU P100 offered by Kaggle, the training should be quite fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T19:08:23.058122Z",
     "iopub.status.busy": "2024-11-16T19:08:23.057715Z",
     "iopub.status.idle": "2024-11-16T20:45:39.674982Z",
     "shell.execute_reply": "2024-11-16T20:45:39.673991Z",
     "shell.execute_reply.started": "2024-11-16T19:08:23.058084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model and the tokenizer are saved to disk for later usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T20:45:39.677954Z",
     "iopub.status.busy": "2024-11-16T20:45:39.677292Z",
     "iopub.status.idle": "2024-11-16T20:45:41.376701Z",
     "shell.execute_reply": "2024-11-16T20:45:41.375732Z",
     "shell.execute_reply.started": "2024-11-16T20:45:39.677906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save trained model and tokenizer\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is the classification report for the evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T20:46:08.119972Z",
     "iopub.status.busy": "2024-11-16T20:46:08.119086Z",
     "iopub.status.idle": "2024-11-16T20:52:04.599574Z",
     "shell.execute_reply": "2024-11-16T20:52:04.598667Z",
     "shell.execute_reply.started": "2024-11-16T20:46:08.11993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict(test, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T20:56:23.084278Z",
     "iopub.status.busy": "2024-11-16T20:56:23.083876Z",
     "iopub.status.idle": "2024-11-16T20:56:23.437409Z",
     "shell.execute_reply": "2024-11-16T20:56:23.436517Z",
     "shell.execute_reply.started": "2024-11-16T20:56:23.084241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation shows good results, compared with the result without fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 622510,
     "sourceId": 1192499,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1918992,
     "sourceId": 3205803,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
